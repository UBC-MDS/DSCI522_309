---
title: "Predicting Online Sales from Webpage Analytics"
author: "Lesley Miller"
date: "24/01/2020"
always_allow_html: true
output: 
  html_document:
    toc: true
    pandoc_args: --webtex
bibliography: citations.bib
---

```{r setup, include = FALSE, warning = FALSE, message = FALSE, echo=FALSE}
library(here)
library(tidyverse, quietly = TRUE)
library(knitr)
library(caret)
library(cowplot)
suppressPackageStartupMessages(library(rlang))
suppressPackageStartupMessages(library(gridExtra))
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_knit$set(root.dir = here::here())
```

```{r load data, message = FALSE, include=FALSE}
cat_vars_expo <- readRDS("results/cat_vars_expo.rds")
quantile_dist <- read_csv("results/quantile_dist.csv")
feature_importance_df <- read_csv("results/Feature_Imp.csv")
X_train <- read_csv("results/Training_pred.csv")
hyperpara <- read_csv("results/GridSearchCV.csv", col_names = F)
roc_train <- read_csv("results/ROC_train_data.csv")
roc_test <- read_csv("results/ROC_test_data.csv")
performance_train <- read_csv("results/metric_by_threshold_train.csv")
performance_test <- read_csv("results/metric_by_threshold_test.csv")
X_train_pred <- read_csv("results/Training_pred.csv")
X_test_pred <- read_csv("results/Test_pred.csv")
```


# Project Summary

Here we attempt to build a classification model using the light gradient boosting algorithm which can use webpage metrics from a given online shopping website to predict whether the revenue of a new customer is True (i.e., the customer purchased something) or False (i.e., the customer did not purchase anything). Our final classifier performed well on an unseen test data set, with the f1 score of more than $0.6$ and the test accuray of ~$90$%. The precision and recall of our classifier on the test set are also around $0.6$. However, because of the small target frequency we have a high percentage of incorrect prediction in FN, we plan to further investigate & improve our model.

# Introduction

## Data Source

The data set used in this project is of online shopping webpage metrics created by C. Okan Sakar, S. Olcay Polat, Mete Katircioglu & Yomi Kastro[@sakar2019real]. It was sourced from the UCI Machine Learning Repository[@Dua] and can be found [here](https://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset). Each row in the data set represents summary behavior extracted from the URL information ,including the final action (purchase or not) and several other measurements (e.g., Number of Distinct Product Related pages, Time spent on Product Related pages, closeness of site visitng time to a special day, etc.). 

## Exploratory Data Analysis


> - Each row represent a session by a user.  
> - Each user has only 1 session in the dataset.  
> - The data is for 1-year period.  
> - ~15% sessions resulted in a purchase.
> - Predictive variables included in this analysis are user's visit information, web analytics features & geographic features.

### Description of the variables, [data source](https://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset)

| No. | Variable | Description |
| ----------- | ----------- | ----------- |
| 1 | `Administrative` | Number of Distinct administrative pages |
| 2 | `Informational` | Number of Distinct Informational pages |
| 3 | `ProductRelated` | Number of Distinct Product Related pages |
| 4 | `Administrative_Duration` | Time(in seconds) spent on Administrative pages |
| 5 | `Informational_Duration` | Time(in seconds) spent on Informational pages |
| 6 | `ProductRelated_Duration` | Time(in seconds) spent on Product Related pages |
| 7 | `BounceRates` | Average bounce rate of all web-pages visited by user. For a web-page its the percentage of people who visit the website from that webpage and left without raising any other request |
| 8 | `ExitRates` | Average exit rate of all web-pages visited by user: For a web-page its the percentage of people who exited the website from that webpage |
| 9 | `PageValues` | Average page value of all web-pages visited by user: For a web-page its the average dollar-value of that page which the user visited before completing the transaction |
| 10 | `SpecialDay` | The closeness of site visitng time to a special day (higher chances of a session resulting in a transaction) |
| 11 | `OperatingSystems` | Operating system used by the user |
| 12 | `Month` | Month of Year |
| 13 | `Browser` | Browser used by the user |
| 14 | `Region` | Geographic region |
| 15 | `TrafficType` | Type of Channel user by the user to arrive at the website |
| 16 | `VisitorType` | Type of the visitor |
| 17 | `Weekend` | Weekend indicator |
| 18 | `Revenue` | Revenue transaction indicator |



```{r numeric variable summary table}

kableExtra::kable_styling(kable(quantile_dist,
      caption = "Cumulative Distribution of Numberic Variables"))
```
![](../results/img/cat_vars_dist_plot.png)

![](../results/img/num_vars_dist_plot.png)

![](../results/img/corr_plot.png)

## Modelling  

We are trying to predict the chances of online purchase by a customer. Since this is a classification problem we tried Machine Learning models that are good for classification and tested their accuracy by optimizing the hyperparameters. We mainly tried Python[@Python]'s Scikit Learn[@scikit-learn]'s [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)(`sklearn.linear_model.LogisticRegression()`), [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)(`sklearn.ensemble.RandomForest()`) & [Light Gradient Boosting](https://lightgbm.readthedocs.io/en/latest/Python-Intro.html)(`lightgbm.LGBMClassifier()`) Technique. The best accuracy on the validation data was achieved by `LGBM`, therefore we went ahead with automating the entire analysis using `LGBM` only.  

During Exploratory Data Analysis(EDA), we used R[@R] and found that the customers who made an online purchase were a small fraction of all the users (~$15$%). Due to this, there was a possibility of facing the class imbalance issue. Because of this, we included the oversampling strategy(`None`: No oversampling, `balanced`:automatically balancing the weights inversely propotional to the class frequencies) provided by `LGBM` in our hyper-parameter optimization and the modeling framework to automatically decide the most optimal oversampling strategy.  

From the initial EDA, we also saw that the variables had a highly skewed distribution with many outliers. Due to this challenge, we preprocessed the data using [Quantile Transformation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html) (`sklearn.preprocessing.QuantileTransformation()`) provided by Scikit Learn[@scikit-learn]. This method transforms the individual explanatory variables to follow normal distribution. This transformation is a robust preprocessing technique and reduces the impact of outliers present in the data.  



```{r feature importance, fig.align="center"}

feature_importance_df %>% 
      filter(Feature_Imp > 40) %>% 
      ggplot(aes(x = reorder(col_names, Feature_Imp), y = Feature_Imp)) + 
      geom_col() +
      labs(title = "Top 10 Most Important Features for Predicting Sales",
           x = "Features",
           y = "Weight of Importance") + 
      coord_flip() +
      theme_bw()
```

The best hyperparameters after grid search is shown below:

```{r best hyperpara}
hyperpara_df <- tribble(
     ~Hyperparameter, ~Value,
     "class_weight", hyperpara$X2[1],
  "max_depth",   hyperpara$X2[2],
  "n_estimators",   hyperpara$X2[3]
)
kableExtra::kable_styling(kable(hyperpara_df,
      caption = "Best hyperparameters chosen"), full_width = F)
```

# Results and Discussion 

For evaluating the performance of the classification models we are using the following metrices:  
- **ROC-AUC Curve**: Area Under the ROC curve is an apt measure of the measuring the performance of the model because it doesn't depend on the threshold chosen for classifying the predicted class.  
- **Confusion Matrix**: Based on the chosen threshold this matrix highlights the classification(True Positives & True Negatives) & misclassification(False Positives & False Negatives) done by the model. The threshold chosen for this matrix is automatically decided by the model based on maximum F1-Score. By looking at the confusion matrix we can know the precision, recall & F1-Score of the predictions which gives a holistic view of the performance of the model.  
- **Effect of Threshold on Performance**: Deciding the threshold of the predicted probability for classification requires human intervention because sometimes the objective is not only maximizing the accuracy. For e.g. a business might want to decide a threshold that minimizes their cost which is a function of (True/False Positive/Negative). Therefore, we are producing this output which highlights how the performance of the model will change by changing the threshold which can help the stakeholders decide on the optimum threshold.  


```{r roc curve, fig.width = 10, fig.align="center"}
roc_curve_train <- roc_train %>% 
   ggplot(aes(x = fpr, y = tpr)) +
   geom_line(size = 1, color = "blue") +
   geom_abline(slope = 1, intercept = 0, linetype = 2, size = 1) +
      labs(title = "ROC Curve on the Train Set",
           x = "False Positive Rate",
           y = "True Positive Rate") +
   annotate("text", x = 0.75, y = 0.3, label = paste("AUC=", round(roc_train$AUC[1],2))) +
   theme_bw()

roc_curve_test <- roc_test %>% 
   ggplot(aes(x = fpr, y = tpr)) +
   geom_line(size = 1, color = "blue") +
   geom_abline(slope = 1, intercept = 0, linetype = 2, size = 1) +
      labs(title = "ROC Curve on the Test Set",
           x = "False Positive Rate",
           y = "True Positive Rate") +
   annotate("text", x = 0.75, y = 0.3, label = paste("AUC=", round(roc_test$AUC[1],2))) +
   theme_bw()

plot_grid(roc_curve_train, roc_curve_test)
```

```{r confusion matrix, fig.width=8, fig.height=4, fig.align="center"}
# get the predicted and observed
predicted <- as.factor(X_train_pred$skl_predict)
observed <- as.factor(X_train_pred$Revenue)
# make the confusion matrix object
confusion_matrix <- confusionMatrix(data = predicted,
                                    reference = observed,
                                    positive = "TRUE")
# extract the confusion matrix table
confusion_matrix_table <- confusion_matrix$table
# convert confusion matrix to tibble
confusion_matrix_tibble <- as_tibble(as.data.frame(confusion_matrix_table))
colnames(confusion_matrix_tibble) <- c("Predicted", "Observed", "Count")
# plot confusion matrix
confusion_train_plot <- confusion_matrix_tibble %>%
      ggplot(aes(x = Observed, y = Predicted)) +
      geom_tile(aes(fill = Count, color = "white")) +
      geom_text(aes(label = Count)) +
      scale_fill_gradient(low = "blue", high = "red") +
      theme(legend.position = "None") +
      labs(title = "Confusion Matrix of the Train") +
      theme(plot.title = element_text(hjust = 0.5, size = 14))

# Test
predicted_test <- as.factor(X_test_pred$skl_predict)
observed_test <- as.factor(X_test_pred$Revenue)
# make the confusion matrix object
confusion_matrix_test <- confusionMatrix(data = predicted_test,
                                    reference = observed_test,
                                    positive = "TRUE")
# extract the confusion matrix table
confusion_matrix_table_test <- confusion_matrix_test$table
# convert confusion matrix to tibble
confusion_matrix_tibble_test <- as_tibble(as.data.frame(confusion_matrix_table_test))
colnames(confusion_matrix_tibble_test) <- c("Predicted", "Observed", "Count")
# plot confusion matrix
confusion_test_plot <- confusion_matrix_tibble_test %>%
      ggplot(aes(x = Observed, y = Predicted)) +
      geom_tile(aes(fill = Count, color = "white")) +
      geom_text(aes(label = Count)) +
      scale_fill_gradient(low = "blue", high = "red") +
      theme(legend.position = "None") +
      labs(title = "Confusion Matrix of the Test") +
      theme(plot.title = element_text(hjust = 0.5, size = 14))

plot_grid(confusion_train_plot, confusion_test_plot)

```
```{r model eval metrics train}
# table of model evaluation scores
confusion_matrix_scores <- tibble(Dataset = c("Train", "Test"),
                                  Precision = c(round(confusion_matrix$byClass[5],3), round(confusion_matrix_test$byClass[5],3)),
                                  Recall = c(round(confusion_matrix$byClass[6],3), round(confusion_matrix_test$byClass[6],3)),
                                  F1_Score = c(round(confusion_matrix$byClass[7],3), round(confusion_matrix_test$byClass[7],3)))
kableExtra::kable_styling(kable(confusion_matrix_scores,
      caption = "Model Evaluation Metrics on Train"), full_width = F)
```

```{r performance, fig.height = 8}
perf_curve_train <- performance_train %>%
   select("f1"=f1_score_train, "precision"=precision_score_train, "recall"=recall_score_train, "threshold"=threshold) %>% 
   gather(key = 'metrics', value = 'score', -threshold) %>% 
   ggplot(aes(x = threshold, y = score)) +
   geom_line(aes(color = metrics), size = 1)+
  labs(
    title = "Model Performance on Train set",
    x = "Threshold",
    y = "Score",
    color = "Type of score"
  ) +
   theme_bw() +
   scale_color_manual(values=c('#999999','#E69F00','#56B4E9'))

perf_curve_test <- performance_test %>%
   select("f1"=f1_score_test, "precision"=precision_score_test, "recall"=recall_score_test, "threshold"=threshold) %>% 
   gather(key = 'metrics', value = 'score', -threshold) %>% 
   ggplot(aes(x = threshold, y = score)) +
   geom_line(aes(color = metrics), size = 1)+
  labs(
    title = "Model Performance on Test set",
    x = "Threshold",
    y = "Score",
    color = "Type of score"
  ) +
   theme_bw() +
   scale_color_manual(values=c('#999999','#E69F00','#56B4E9'))
plot_grid(perf_curve_train, perf_curve_test, nrow = 2)
```

# References